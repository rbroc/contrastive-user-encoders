{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW baseline\n",
    "Computing baselines (training tokenizer on first 10 batches) on 50 batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import glob\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = glob.glob('../reddit/data/json/triplet/1pos_1neg_random/train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_posts = []\n",
    "for f in fs[:10]:\n",
    "    t_data = json.load(gzip.open(f))\n",
    "    for t in t_data:\n",
    "        t_posts += t['anchor']\n",
    "        t_posts += t['negative']\n",
    "        t_posts += t['positive']\n",
    "t_posts = [' '.join(t.split(' ')[:400]) for t in t_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = [100, 1000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in num_words:\n",
    "    tokenizer = Tokenizer(num_words=n)\n",
    "    tokenizer.fit_on_texts(t_posts)\n",
    "    pkl.dump(tokenizer, \n",
    "             open(f'../reddit/preprocessing/triplet_baselines/tokenizer_{n}.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw-langmod",
   "language": "python",
   "name": "tw-langmod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
